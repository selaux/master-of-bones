\documentclass[pdftex,12pt,a4paper]{report}
\usepackage{dbstmpl}
\usepackage{subfigure}
\usepackage[utf8]{inputenc}

% Hier die eigenen Daten eintragen
\global\arbeit{Masterarbeit}
\global\titel{Data Science in Zooarchaeology: Analyzing local shape variations in bone findings}
\global\bearbeiter{Stefan Lau}
\global\betreuer{Johannes Niedermayer}
\global\aufgabensteller{Dr. Matthias Renz}
\global\abgabetermin{31.08.2015}
\global\ort{MÃ¼nchen}
\global\fach{Medieninformatik}

\begin{document}

% Deckblatt
\deckblatt

% Erklaerung fuer das Pruefungsamt
\erklaerung

% Zusammenfassung
\begin{abstract}
Dieses Dokument dient als Muster f"ur die Ausarbeitung einer \the\arbeit\
an der Lehr- und Forschungseinheit f"ur Datenbanksysteme am Institut f"ur
Informatik der LMU M"unchen.
\end{abstract}

% Inhaltsverzeichnis
\tableofcontents

% Hier beginnt der eigentliche Text
\chapter{Introduction}

\chapter{Problem Definition}

\chapter{Related Work}

\section{Morphometrics in Zooarcheology}

\cite{blackith1971multivariate}
\cite{adams2004geometric}
\cite{mitteroecker2009advances}

\section{Shape Matching in Data Science}

\cite{da2010shape}
\cite{veltkamp2001shape}
\cite{belongie2002shape}
\cite{mhamdi2014local}

\chapter{Mathematical Basics}

\section{Image Segmentation}

\cite{van2014scikit}

\section{Splines}
\label{section:splines}

\section{Support Vector Machines}

\cite{pedregosa2011scikit}

\section{Decision Trees}

\chapter{Detection of Local Shape Variations}

\section{Overview}

\section{Preprocessing}

\subsection{Image Segmentation}
\label{sub:segmentation}

\cite{achanta2012slic}
\cite{felzenszwalb2004efficient}
\cite{houhou2009fast}
\cite{jain1990unsupervised}
\cite{haralick1973textural}

\subsection{Triangulation and Outlining}

The next step for preprocessing images was extracting the outline from a segmented image. As stated in Section
\ref{sub:segmentation}, we marked some pixels in the images as being pixels that lie on the bone.
To extract the outline from these pixels, we first transformed them into points in the x-y-space and then
triangulated them using the delaunay triangulation.

To reduce the number of outliers that were detected as on-bone-pixels,
we decided to use the DBSCAN algorithm from \cite{ester1996density} and apply density based clustering to the points
beforehand. DBSCAN allowed us to create clusters from the data, which correspond to closely packed points. Since
the points on the bone were closely packed, and the points not on the bone were only sparsely detected as
on-bone-pixels this allowed us to filter some of the false-positives. Another benefit of the DBSCAN algorithm is that,
by defining our bone as a large densely connected area, we were able to use the largest cluster from the result,
so closely packed areas that are disconnected from the bone were also removed.

The delaunay triangulation was applied to the remaining points to create a triangulated mesh. Since delaunay
triangulation can only create meshes with convex outlines, another step was required to extract the bone outline
from the triangulation. For this purpose we used the $\alpha$-complex of the mesh as defined in \cite{akkirajualpha}.
The $\alpha$-complex algorithm removes outer triangles for the Delaunay triangulation that have a radus which exceeds
$\frac{1}{\alpha}$. $\alpha$ is a parameter and was chosen as $25$ for our purpose, which provided us with
an accurate concave outline of the bone.

Since we are analyzing the shape of the object, a manual step was introduced here to give the user the ability to
approve the automatically extracted outline and adapt it if the result was not satisfactory. We gave the user the
ability to mark points as on-the-bone/off-the-bone as well as fill a rectangle with a grid of on-the-bone points (or mark them all as not on-the-bone).
This allows the user to easily adapt a triangulation and, if necessary, create a completely new one. It becomes
especially important because the result of the image segmentation algorithm often has jagged borders, because the
image segmentation algorithms do not consider the smoothness of the outline.

Since the bones might have different sizes and positions, due to different image sizes and position of the bone in
the image, we needed to introduce another normalization step afterwards. We decided to remove all variance in between the bones that are not attributed to shape. For this purpose we executed the first two steps of Procrustes
analysis, also used in Section \ref{subsub:procrustes}.

\begin{itemize}
	\item Transformation of all point sets into the coordinate center
	\begin{equation}
		\begin{split}
			& \bar{x} = \frac{x_{1x} + x_{2x} + \cdots + x_{nx}}{n} \\
			& \bar{y} = \frac{x_{1y} + x_{2y} + \cdots + x_{ny}}{n} \\
			& X_c = \{ (x_{1x} - \bar{x}, x_{1y} - \bar{y}), \cdots, + (x_{nx} - \bar{x}, x_{ny} - \bar{y}) \}
		\end{split} 
	\end{equation}
	\item Scaling all point sets uniformly
	\begin{equation}
		\begin{split}
			& s = \sqrt{\frac{\sum\limits_{x \in X_c}||x||^2}{n}} \\
			& X_s = \left\{ \frac{x_{c1}}{s}, \cdots, \frac{x_{cn}}{s}\right\}
		\end{split}
	\end{equation}
\end{itemize}

\subsection{Landmark Extraction}
\label{sub:landmarks}

As some of the features we propose in this work are landmark based, and the detection of on-site-measurable differences
is based on landmarks as well, a method to extract these landmarks from the outline of the bone is needed. In contrast to
our approach, former work on the talus by zooarcheologists was based on manually located landmarks that correspond to
biological features of the bone. Since our goal was to automate the process as much as possible and some of the landmarks
are not positioned on the outline of the bone, this was not feasible in our case. We selected landmarks on the outline
of the bone that could be located automatically and overlap as much as possible with the landmark definitions by the
zooarcheologists. For this purpose we introduced two methods that extract landmarks from outlines of Tali. The landmarks
found by these methods have very similar positions but differ in on-site-measurability and stability in-between classes.
Figure \ref{figure-landmark-methods} shows the results of the three landmark locating methods: manual, with angles and
with space partitioning. The biological correspondences of the landmarks are as follows.

\begin{itemize}
\item Landmark 1: Highest point on the lateral crest
\item Landmark 2: Lowest point in between the crests
\item Landmark 3: Highest point on the medial crest
\item Landmark 4: Intersection between the medial outer edge and the medially arcing edge of the medial crest
\item Landmark 5: Out most point on the medial apex
\item Landmark 6: Lowest point on the medial outer edge
\item Landmark 7: Highest point on the distal depression
\item Landmark 9: Intersection of the distal articular surface with the Calcaneus
\item Landmark 10: Lateral end point of the lower muscle attachment line
\item Landmark 11: Intersection of the two muscle attachment lines
\item Landmark 12: Proximal end point of the lateral protrusion on the lateral crest
\end{itemize}

The landmarks that were extracted automatically are landmarks 1, 2, 3, 5, 6, 7. Furthermore we introduced the new
landmark 8, which has no biological correspondence, but can be located robustly on all bone outlines.

\subsubsection{Extraction by Angle}

When extracting a landmark by angle, the landmark position is defined by a minimum angle $\alpha_{min}$, a maximum
angle $\alpha_{max}$ and the information whether it is located on a minimum or a maximum turning point.
The minimum/maximum turning angle $\alpha_{t}$ between $\alpha_{min}$ and $\alpha_{max}$ of the following function
is calculated, where $i(\alpha)$ is the intersection between a ray from the coordinate center with the angle $\alpha$ and
the outline. $i(\alpha_t)$ is then used as the landmark.

\begin{equation}
d(\alpha) = ||i(\alpha)||
\end{equation}

The definitions for all landmarks when using this method can be found in Appendix \ref{appendix:table:landmarks-angle}.

The landmarks found by this method are robustly located across all classes, but have a distinct disadvantage. They are
not as easily located by hand since they dont lie exactly on the high points of the outline, but slightly beneath
them  as seen in Figure \ref{figure-landmark-methods}. This makes distances inbetween landmarks harder to measure, which
will become relevant in Chapter \ref{chapter:measurable-differences}.

\subsubsection{Extraction by Space Partitioning}

When extracting a landmark by space partitioning, the landmark position is defined by a rectangle in which the landmark
lies, the information whether it lies on a maximum or a minimum turning point and the coordinate axis on which the
turning point lies. The rectangle is defined by $x_{min}$, $x_{max}$, $y_min$ and $y_max$. The turning point
is then found by only evaluating the respective axis inside the rectangle. The following function for turning points
is then evaluated inside the rectangle, where $s_y(x)$ is the x-coordinate of the spline at the x-coordinate $x$.

\begin{equation}
d(x) = s_y(x)
\end{equation}

The definitions for all landmarks when using the space-partitioning method can be found in Appendix
\ref{appendix:table:landmarks-space}.

When looking at the landmarks found by this method, they tend to lie exactly on the high points of the outline, which
makes them easily locatable by hand. The drawback of this approach is that local high or low points can be mistaken as
landmarks by this algorithm, especially when the bone shapes are not registered yet.

\section{Shape Registration}

To adequately compare the shapes, we needed to align them first as much as possible, so small differences in shape would
show in the result. For this purpose we implemented several methods for point-to-point registration algorithms in order
to find the one that best matches our dataset. In general point-to-point registration algorithms that matches point set
$X$ onto point set $Y$ consist of two parts:

\begin{itemize}
\item Find correspondent points $C_Y$ for (some of the) points in $X$
\item Find transformation $T$ that aligns $X$ to $C_Y$ as best as possible under certain restrictions
\end{itemize}

We chose the reference point set $Y$ to be represented by the bone outline with the most points, since this will allow
us to find non-duplicate correspondences from $X$ to $Y$. Since we are considering the outline of the bone and not
individual points, we can choose any points on the outline $B_Y$ as $Y$ and any point on the outline $B_X$ as $X$.
Transformation estimations can also easily be iterated as done by the ``Iterative Closest Point`` method \cite{besl1992method}.
The method can be different in each iteration. We implemented several methods for point-to-point correspondence
approaches as well as several for the estimation of the transformation which have different restrictions.

\subsection{Correspondence Estimation}

Corresponding points are defined as pairs of semantically identical points in both point sets. Based on these
pairs, the transformation estimation is done.

\subsubsection{Landmarks}

When estimating the correspondence using landmarks, the landmark points are located as described in Section
\ref{sub:landmarks}. Since the number of landmarks is always the same in both outlines, we used the result
as one-to-one correspondences for the registration. We implemented both the angle-based as well as the
space-partitioning-based approaches of the automated landmark extraction. Additionally, we added a method
that evaluates the manually set landmarks of the bones.

\subsubsection{Spline Points}

Another approach to find correspondences was to parameterize the bone outline using a spline and then using $n$
evaluated points on the spline as a normalized representation of the shape. The spline representation consists
of two functions $s_x$, $s_y$ for the $x$ and $y$ coordinates of the outline. We evaluated this spline using $n$
equally spaced parameters $T$ as seen in Equation \ref{eq:registration-spline}.

\begin{equation}
\label{eq:registration-spline}
\begin{split}
& s(t) = ( s_x(t), s_y(t) ) \text{ with } 0 \leq t \leq 1 \\
& T = \{ t_i=\frac{i}{n} | i=0, 1, \dots, (n-1) \} \\
& X = \{ s(t_i) | t_i \in T \}
\end{split}
\end{equation}

These spline points can be evaluated for all bones in the bone set and provide a one-to-one estimation of correspondences.

\subsubsection{Nearest Neighbors}

All previously suggested approaches have one distinct disadvantage: The correspondences are static, meaning they don't
change after an iteration (although the positions of the automatically found landmarks might change). To compensate this
we implemented a nearest-neighbor approach to estimate correspondences. The resulting algorithm is very similar to
\cite{besl1992method} but uses other transformation estimators.

To find the correspondences for $X$ in $Y$ we simply use the nearest neighbor $NN_Y(x_i)$ for each point $x_i$ in $X$.

\begin{equation}
NN_Y(x_i) = \arg\min(\{ \forall y_i \in Y: dist(x_i, y_i) \})
\end{equation}

The correspondences found with this method might change after each iteration. This is beneficial for already registered
outlines.

\subsection{Transformation Estimation}

The transformation estimation is done for the selected corresponding points only and then applied to all points in
the point set.

\subsubsection{Procrustes Transformation}
\label{subsub:procrustes}

Procrustes transformation is a transformation that uses a combination of translation, scaling and rotation. This kind of
transformation preserves the original shape of the object, making it a similarity transformation. In our case reflection
was not necessary since all the objects were previously aligned already. The Procrustes transformation is closely related to the
Procrustes analysis. To superimpose a point set $X$ onto a reference point set $Y$ three steps are necessary. The first
two steps (transformation into center and uniform scaling) were already done in the normalization step. The only
estimation necessary is the one of the angle $\theta$ which the point set $X$ needs to be rotated by to get the registered
point set $R$.

\begin{equation}
\begin{split}
& \theta = \arctan{\left( \frac{\sum\limits_{i = 1}^n(x_{ix}y_{iy} - x_{iy} y_{ix})}{\sum\limits_{i = 1}^n (x_{ix} y_{ix} + x_{iy} y_{iy}) } \right)} \\
& \vec{r_i} = { (x_{ix} \cos\theta - x_{iy} \sin\theta, x_{ix} \sin\theta - x_{iy} \cos\theta) } \\
& R = \{ \vec{r_1}, \cdots, \vec{r_n} \}
\end{split}
\end{equation}

When executing the Procrustes transformation in an iterative manner using differing correspondence estimations,
translation and scaling need to be re-estimated as well.

\subsubsection{Affine Transformation}
\label{subsub:affine}

An affine transformation is a transformation between two spaces that preserves three properties:

\begin{itemize}
\item Collinearity: All points on a line still lie on a line after the transformation
\item Parallelism: Two parallel lines are still parallel after the transformation
\item Proportions of distances: Three points on a line have the same ratio of distances after the transformation
\end{itemize}

Estimating kind of transformation allows for more flexibility to align the shapes. At the same time it has the
disadvantage of actually changing the shape of the object. The Procrustes transform we presented in Section
\ref{subsub:procrustes} is a subset of the affine transformation. Other transformations include shearing and combination of all presented transformations. Since the bones were sometimes photographed from
slightly different perspectives, we allowed the slight change in shape in this case, especially because was
applied to the whole object.

A affine transformation is represented by a $3\times3$ transformation matrix $A$. A affine transform that
superimposes a point set $X$ onto a point set $Y$ can be written in homogeneous coordinates as:

\begin{equation}
\label{eq:projectivetransformation}
\begin{split}
& \vec{r_i} = A \vec{x_i} \\
& R = \{ \vec{r_i}, \cdots, \vec{r_n} \}
\end{split}
\end{equation}

The estimation of the affine transform was done using the scikit-image python library \cite{van2014scikit}.
It uses a linear equation system to solve for $A$ minimizing the mean squared error between $R$ and $Y$.

\subsubsection{Projective Transformation}

A projective transformation, often referred to as homography is another kind of transform we implemented.
Projective transformations are all transformations that adhere to the collinearity constraint introduced
in Section \ref{subsub:affine}. Since projective transformations are transformations specifically used
to study perspective, we used it to correct for differing perspectives of the bone photographies.

A projective transformation can be represented similarly to an affine transform as defined in Equation
\ref{eq:projectivetransformation}. The estimation was done by scikit-learn \cite{van2014scikit} as well. 

\section{Shape Comparison}

After registering the bone shapes, we could test them for significant local shape variations.
To represent the outlines we decided to use splines as introduced in Section \ref{section:splines},
as this allows us to easily extract segments of the bone outline. After the registration all outlines
adhere to the following standards.

\begin{itemize}
\item The centroid lies close to the coordinate center $(0,0)$
\item The expansion lies within the ranges of $0$ to $2$ in the x and y directions
\end{itemize}

To further normalize the data we calculated the intersection between the x-axis and the bone outline
in the positive section of the x-axis. This point was introduced as the starting point for the outline.
The spline $(s_x(t), s_y(t))$ for the interval $t \in [0, 1]$ was then extracted leading to the
following standards for the the spline representation for each bone. These standards can be leveraged for the comparison algorithm.

\begin{equation}
\begin{split}
& s_y(0) = 0 \\
& s_y(1) = 0 \\
& s_x^\prime(0) = s_x^\prime(1) \\
& s_y^\prime(0) = s_y^\prime(1)
\end{split}
\end{equation}

To compare the shapes, we evaluated windows around $n$ points on the outline. Each evaluation
point therefore represents a neighborhood around itself. For each of these neighborhoods
we calculated a set of features for each bone in the database. Then a support vector machine
was used to determine the separability of the established classes at these points. Based on 
these separability measures, we can then decide whether the two classes can be distinguished
at this point or not. 

\subsection{Selection of Evaluation Windows}

The next step in the algorithm is the selection the evaluation windows at an angle $\rho$ for
each bone $B \in DB$. For this purpose we implemented two approaches to the problem.

\subsubsection{By Angle}
\label{subsub:windowbyangle}

The first approach to select an evaluation window by cutting a section of a certain angular
width from the bone outline. The window in this method is defined by $\rho$ and $\Delta\rho$
and is derived by ray casting from the coordinate center at the angles $\rho - \Delta\rho$ and 
$\rho + \Delta\rho$ and using the polygon segment that lies in between these intersection points. 

To find an intersection point at a certain angle $\rho$, the spline $\vec{s_B}(t) = (s_{Bx}(t), s_{By}(t))$ was evaluated at $k=250$ evenly spaced
parameters $t_i \in [0,1]$ for each bone $B \in DB$. This evaluation was joined to a polygon
with $k$ line segments $s_i$. The intersection $\vec{i_B}(\rho)$ between this polygon and the ray was calculated and
the line segment $s_i$ that had this intersection on it was selected. $s_i$ is the line
segment between $\vec{s_B}(t_i)$ and $\vec{s_B}(t_{i+1})$ or $\vec{s_B}(0)$ for $i=k$.

To extract the exact parameter $t_{int}(\rho)$ where the intersection occurred, The distances
between the intersection $\vec{i_B}(\rho)$ and the endpoints of the line segment $s_i$ were put into relation.

\begin{equation}
t_{int}(\rho) =
\frac{|\vec{i_B}(\rho) - \vec{s_B}(t_{i+1})|}{|\vec{s_B}(t_i) - \vec{s_B}(t_{i+1})|} t_i +
\frac{|\vec{i_B}(\rho) - \vec{s_B}(t_i)|}{|\vec{s_B}(t_i) - \vec{s_B}(t_{i+1})|} t_{i+1}
\end{equation}

This leads to a good estimation of $t_{int}$ and the intersection point $\vec{s_B}(t_{int})$
for each bone $B$ at the angle $\rho$.

Performing this calculation for $\rho-\Delta\rho$ and $\rho+\Delta\rho$ leads to two spline
parameters $t_{\rho-\Delta\rho}$ and $t_{\rho+\Delta\rho}$ in between the spline can be 
evaluated to get the evaluation window.

The drawback of this method is that bones that are not aligned very well get shorter windows
if their outline lies closer to the coordinate center and longer windows otherwise. This 
might lead to unwanted results when doing the comparison since the considered windows are
not semantically equal. 

\subsubsection{By Length}

To nullify this drawback, we implemented another method to extract the evaluation window.
In this approach, a window of a certain length $l$ around a point of evaluation that is defined by the angle $\rho$ is extracted. To find these points we employed ray-casting
as introduced in Section \ref{subsub:windowbyangle}. This leads to a spline parameter
$t_\rho$ which now defines the center of our window.

To get a window from $t_\rho$, we expand the initial window $[t_{rho}-\Delta t_i,
t_{rho}+\Delta t_i]$ with $\Delta t_0 = 0$ and $i=0$ iteratively. With each iteration we add a constant value $\delta$ to $\Delta t_i$.

\begin{equation}
\Delta t_i = \Delta t_{i-1} + \delta
\end{equation}

When the window length is equal, or exceeds the required length $l$, we abort and get the current
window as the result of our calculation.

\begin{equation}
\int_{t_\rho - \Delta t_i}^{t_\rho + \Delta t_i} ||\vec{s_B}(t)|| dt \leq l
\end{equation}

Similarly to the angle based method presented in Section \ref{subsub:windowbyangle}, this leads to the spline parameters $[t_{rho}-\Delta t_i, t_{rho}+\Delta t_i]$ of the last iteration $i$, in between which the window at angle $\rho$ of length $l$ is defined.

The windows extracted by this method have approximately (because of the discrete
characteristic of the algorithm) the same length, making them more similar across
the different bones.

\subsection{Feature Extraction}
\label{sub:featureextraction}

To compare the two classes at an angle $\rho$, we needed to extract features from the
window defined by $[t_{start}, t_{end}]$ for each bone. We evaluated several feature
extraction methods for this purpose. Most of these methods operate on a set of points
$S$ that are created by evaluating the spline at $k$ equally spaced parameters.

\begin{equation}
\begin{split}
& T = \{ t_i=t_{start} + \frac{i}{n} * (t_{end}-t_{start}) | i=0, 1, \dots, (k-1) \} \\
& S = \{ \vec{s}(t_i) | t_i \in T \}
\end{split}
\end{equation}  

Other features like the distances to the markers only use a single point, the center of
the window.

\subsubsection{Flattened Points}

The simplest feature vector we used is to use a flattened version of $S$ as the feature
vector. We consider this the baseline of our algorithm.

\begin{equation}
\vec{f} = \left( \begin{array}{c}
s_x(t_0) \\
s_y(t_0) \\
\vdots \\
s_x(t_{k-1}) \\
s_y(t_{k-1})
\end{array} \right)
\end{equation}

\subsubsection{Distance to Center}

Another feature based on the point set $S$ is the distance of each point $s(t_i)$ to the
center. This feature was considered to be more robust because it offers a lower dimensionality
than to simply flatten the points.

\begin{equation}
\vec{f} = \left( \begin{array}{c}
||s(t_0)|| \\
\vdots \\
||s(t_{k-1})||
\end{array} \right)
\end{equation}

\subsubsection{Distance to Center and Curvature}
\label{subsub:featuredistancetocenterandcurvature}

To additionally take the relationships in between the points into account we considered
to take the curvature of the spline evaluation into account as well. The curvature $\kappa$ of the spline at the parameter $t_i$ can be calculated as following.

\begin{equation}
\kappa(t_i) = \frac{|s_x^\prime(t_i) s_y^{\prime\prime}(t_i) - s_y^\prime(t_i) s_x^{\prime\prime}(t_i)|}{(s_x^\prime(t_i)^2 + s_y^{\prime}(t_i)^2)^{3/2}}
\end{equation}

The feature vector then consists of a combination of distances and the curvature of the
corresponding points.

\begin{equation}
\vec{f} = \left( \begin{array}{c}
||s(t_0)|| \\
\kappa(t_0) \\
\vdots \\
||s(t_{k-1})|| \\
\kappa(t_{k-1})
\end{array} \right)
\end{equation}

\subsubsection{Curvature of Distance to Center}

For this feature the idea was to only use the relationship between the points in $S$ and describe
the polygon section by the curvature $\kappa$ of the distances to the center. Since the distance 
to the center is no longer a parameterized curve the equation for $\kappa$ differs from
Section \ref{subsub:featuredistancetocenterandcurvature}.

\begin{equation}
\begin{split}
& d(t_i) = ||s(t)_i|| \\
& \kappa(t_i) = \frac{|d^{\prime\prime}(t_i)|}{(1 + d^\prime(t_i)^2)^{2/3}}
\end{split}
\end{equation}

The feature vector is then composed of the curvature of the points in the point set $S$.

\begin{equation}
\vec{f} = \left( \begin{array}{c}
\kappa(t_0) \\
\vdots \\
\kappa(t_{k-1})
\end{array} \right)
\end{equation}

\subsubsection{Derivation from Mean Bone}

The concept of this feature was to use the fact that the mean bone outline is derived from both classes.
Differences in the shape of the two classes should show in derivations from the mean bone outline that
occur in different directions. For this purpose we need to calculate the mean bone $\bar{B} = \{ (\bar{b}_{x0}, \bar{b}_{y0}), \cdots, (\bar{b}_{xk-1}, \bar{b}_{yk-1}) \}$ for the current window of all bones $B \in DB$ first. $(s_{xB}(t), s_{yB}(t))$ defines the spline representation and $T_B = \{ t_{B0}, \cdots t_{Bk-1} \}$ is the extracted window for each bone $B$. The feature vector is then defined by the difference between the mean bone and
each evaluation of the spline.

\begin{equation}
\begin{split}
& \bar{b}_{xi} = \frac{1}{|DB|} \sum_{B \in DB} s_{xB}(t_{Bi}) \\
& \bar{b}_{yi} = \frac{1}{|DB|} \sum_{B \in DB} s_{yB}(t_{Bi}) \\
& \vec{f}_B = \left( \begin{array}{c}
s_{xB}(t_{B0}) - \bar{s}_{x0} \\
s_{yB}(t_{B0}) - \bar{s}_{y0} \\
\vdots \\
s_{xB}(t_{Bk-1}) - \bar{s}_{xk-1} \\
s_{yB}(t_{Bk-1}) - \bar{s}_{yk-1} \\
\end{array} \right)  
\end{split}
\end{equation}

\subsubsection{Spline Derivatives}

Spline derivatives can be used to analyze the shape of an object as well. Changes in the shape reflect heavily
on the derivatives since they show the direction the outline is currently bending to. To filter out high frequency
changes which occur due to the uneven surface of the bone, smoothing based on the Fast Fourier Transform is applied
beforehand. The smoothing acts as a low-pass filter in frequency space.

TODO: Howto FFT Splder

\subsection{Dimensionality Reduction}

Depending on the number of evaluations $k$ in the feature extraction step, shown in Section
\ref{sub:featureextraction}, the dimensionality of the feature vector for each bone is relatively high. Most of 
the feature vectors have a dimensionality $d = 2k$ or $d = k$. Depending on the value of $k$, this might become
a feature vector of $> 50$ dimensions.

To further analyze these features, we decided to introduce an optional step where principal component analysis
(PCA) is applied to find the main components of the feature vectors. PCA transforms a number of correlated variables into a number of uncorrelated variables called principal components. The number of principal components can then be limited to to reduce the dimensionality of the feature set.

Scikit-learn \cite{pedregosa2011scikit} provides an implementation of PCA, which we used to reduce the number of features to a user-defined number.

\subsection{Calculation of Separability Measures}

After extracting features $f_B$ for each bone $B \in DB$ at a certain evaluation point, the last step in the
algorithm is to calculate an indicator that shows whether the two classes can be separated well around this point.
This indicator can be used to show the location of the characteristic differences on the bone outline. For this
purpose we used a Support Vector Machine to calculate several separability measures.

\subsubsection{Margin}

The SVM has the advantage of having the first separability measure built in: The width of the margin between the
support vectors and the separating hyperplane $D$. Since the soft margin method is used to train the SVM, allowing for incorrectly labeled observations, this is the distance to the nearest cleanly split examples. The margin is 
defined using the normal vector of the separating hyperplane $\vec{w}$. We used all observations $TR = \{ f_B \forall B \in DB \}$ to train the SVM $K$ in this case.

\begin{equation}
m = \frac{2}{||\vec{w}||}
\end{equation}

\subsubsection{Observed Accuracy}

Since the margin might incorporate incorrectly classified examples, we decided to use other separability measures
as well to confirm the results defined by the margin. For this purpose we used the observed accuracy metric of the SVM classifier $K$, trained with all observations as well. The class of the observation is here denoted as $C(o)$
while the predicted class is denoted as $K(o)$.

\begin{equation}
a_{TR}(K) = \frac{|o \in TR | K(o) = C(o)|}{|TR|}
\end{equation}

\subsubsection{Mean Confidence Score}

Using the SVM for classification also allowed us to see how well an observation can be assigned to a class. This
is called a confidence score. The confidence score for the SVM is the distance from the observation to the dividing
hyperplane $D$. We can use this to calculate a mean confidence score for all observations for the current evaluation point.

\begin{equation}
c_{TR}(K) = \frac{1}{|TR|} (\sum_{o \in TR | K(o) = C(o)} dist(o, D) - \sum_{o \in TR | K(o) \neq C(o)} dist(o, D))
\end{equation}

\subsubsection{Mean Cross-Validation Accuracy}

In classification applications it's usually beneficial to separate the training and test sets to see how well a
classifier can be generalized. Since our application is not a classifier but still very close to it, we wanted
to make sure that the SVM could be generalized as well. When the classifier can be generalized well, the separation
should be good as well at this position.

Since our dataset is relatively small, we decided to use cross-validation to verify that the classifier can be
generalized. In cross-validation the dataset is split up into $k$ disjoint sets $S_i$ that hold $\frac{|DB|}{k}$ items each. For our case we used $4$ items per set in a stratified manner. Stratified cross-validation is done by using the same ratio of classes inside each disjoint set as inside the whole database. The cross-validation is then
executed by using all but one set as the training data and the remaining one as the test data. This is done $k$ times, so each set is used as test data once.

\begin{equation}
\begin{split}
& TR_i = DB \setminus S_i \\
& TE_i = S_i 
\end{split}
\end{equation}

Using these sets $k$ SVMs $K_i$ are trained. The mean accuracy on the test sets of each classifier is then used
as the measure.

\begin{equation}
\bar{a} = \frac{1}{|DB|} \sum_{i=1}^k | \{o \in TE_i | K(o) = C(o) \}| 
\end{equation}



\chapter{Detection of Differences That Are Measurable On-Site}
\label{chapter:measurable-differences}

\section{Definition of Measurable Values}

\section{Training Decision Trees}

\chapter{Experiments}

\section{Tests on Real-World Data}

\subsection{Results and Compliance with Former Findings}

\subsection{Comparison of Algorithm Parameters}
\label{sub:comparisonalgorithmparameters}

\subsection{Comparison of Features}

\subsection{Comparison of Registration Strategies}

\subsection{Comparison of Separability Measures}

\section{Tests on Synthetic Data}

\subsection{Generating Synthetic Data}

\subsection{Influence of Variances in Shape Features}

\subsection{Influence of Errors in Shape Measurement}

\subsection{Influence of Errors in Shape Registration}

\chapter{Conclusion}

\section{Summary}

\section{Discussion}

\section{Future Work}

\appendix

\chapter{Landmark Extraction}

\begin{table}
    \begin{center}
        \begin{tabular}{|r|r|r|r|}
            \hline \makebox[3cm]{Landmark No} & \makebox[1cm]{$\alpha_{min}$} & \makebox[1cm]{$\alpha_{max}$} & \makebox[2cm]{Type} \\
            \hline\hline 1 &  $30^{\circ}$ & $90^{\circ}$ & maximum \\
            \hline 2 &  $80^{\circ}$ & $100^{\circ}$ & minimum \\
            \hline 3 &  $90^{\circ}$ & $150^{\circ}$ & maximum \\
            \hline 5 &  $170^{\circ}$ & $190^{\circ}$ & maximum \\
            \hline 6 &  $210^{\circ}$ & $270^{\circ}$ & maximum \\
            \hline 7 &  $260^{\circ}$ & $280^{\circ}$ & minimum \\
            \hline 8 &  $270^{\circ}$ & $330^{\circ}$ & maximum \\
            \hline
        \end{tabular}
    \end{center}
    \caption{Landmark Definitions for the Extraction by Angle.}
    \label{appendix:table:landmarks-angle}
\end{table}

\begin{table}
    \begin{center}
        \begin{tabular}{|r|r|r|r|r|r|r|}
            \hline
            \makebox[3cm]{Landmark No} & \makebox[1cm]{$x_{min}$} & \makebox[1cm]{$x_{max}$} & \makebox[1cm]{$y_{min}$} & \makebox[1cm]{$y_{max}$} & \makebox[2cm]{Axis} & \makebox[2cm]{Type} \\
            \hline
            \hline 1 &  $0.25$ & $1$ & $0.75$ & $1.5$ & y & maximum \\
            \hline 2 &  $-0.5$ & $0.5$ & $0.75$ & $1.5$ & y & minimum \\
            \hline 3 &  $-1$ & $-0.25$ & $0.75$ & $1.5$ & y & maximum \\
            \hline 5 &  $-1$ & $-0.25$ & $-0.3$ & $0.3$ & x & minimum \\
            \hline 6 &  $-1$ & $0.25$ & $-1.5$ & $-0.75$ & y & minimum \\
            \hline 7 &  $-0.5$ & $0.5$ & $-1.5$ & $-0.75$ & y & maximum \\
            \hline 8 &  $0.25$ & $1$ & $-1.5$ & $-0.75$ & y & minimum \\
            \hline
        \end{tabular}
    \end{center}
    \caption{Landmark Definitions for the Extraction by Space Partitioning.}
    \label{appendix:table:landmarks-space}
\end{table}

% Abbildungsverzeichnis (kann auch nach dem Inhaltsverzeichnis kommen)
\listoffigures

% Tabellenverzeichnis (kann auch nach dem Inhaltsverzeichnis kommen)
\listoftables

% Literaturverzeichnis
\bibliographystyle{dbstmpl}    % verwendet dbstmpl.bst
% alternative, vorinstallierte Stile sind z.B. plain oder abbrv
\bibliography{dbstmpl}         % verwendet dbstmpl.bib

\end{document}
